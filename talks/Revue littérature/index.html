<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Deep and second Second-order</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="css/theme/metropolis.css" id="theme">
		<link rel="stylesheet" href="plugin/title-footer/title-footer.css">
		<link rel="stylesheet" href="plugin/chalkboard/style.css">
		<link rel="stylesheet" href="css/flexbox/flexboxgrid.css" type="text/css">
		<link rel="stylesheet" href="plugin/presentable/presentable.min.css">
		<link rel="stylesheet" href="css/custom.css" type="text/css">


		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
		<link rel="stylesheet" href="plugin/verticator/verticator.css">
	</head>
	<body>

		<div class="reveal">
			<div class="slides">

<!-- --------------------------------------------------------------------------------------------- -->
<!-- -------------------------- THE PRESENTATION ELEMENTS GO UNDER HERE -------------------------- -->
<!-- --------------------------------------------------------------------------------------------- -->
				
				<!-- First horizontal slide -->
				<section>


						<div>
							<h1 class="talk-title" style="font-weight: bolder;">Deep learning and second order</h1>
							<h2>A humble and brief review...</h2>
						</div>
						<div>
							<iframe scrolling="no" style="width: 400px; height: 400px;" src="./content/animation_desk.html"></iframe>
						</div>


				</section>
				<!-- End of first horizontal slide -->

				<section>
					<h1>Table of Contents</h1>
					<nav id="presentable-toc" class="revealjs"></nav>
				</section>

				<!-- Second horizontal slide -->
				<section>

					<!-- First vertical slide on second horizontal slide-->
					<section><h1>References and misc.</h1></section>

					<section>
						<h2>Existing fields</h2>
						<iframe style="border:none" width="100%" height="100%" data-src="./assets/map.html" data-preload></iframe>
					</section>

					<section>
						<h2>List of publications</h2>

						<div style="height: 550px; width: 1300px; overflow: auto; position: absolute;
						left: -150px;
						padding: 0;
						margin: 0;">
						   <style type="text/css">
						   .tg  {border:none;border-collapse:collapse;border-spacing:0;}
						   .tg td{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;
							 padding:10px 5px;word-break:normal;}
						   .tg th{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;
							 overflow:hidden;padding:10px 5px;word-break:normal;}
						   .tg .tg-4fps{background-color:#efefef;border-color:#000000;color:#000000;text-align:left;vertical-align:top}
						   .tg .tg-tf2e{text-align:left;vertical-align:top}
						   .tg .tg-0lax{text-align:left;vertical-align:top}
						   </style>
						   <table class="tg" style="undefined;table-layout: fixed; width: 1244px">
						   <colgroup>
						   <col style="width: 278px">
						   <col style="width: 202px">
						   <col style="width: 233px">
						   <col style="width: 153px">
						   <col style="width: 82px">
						   <col style="width: 141px">
						   <col style="width: 155px">
						   </colgroup>
						   <thead>
							 <tr>
							   <th class="tg-4fps">Titre</th>
							   <th class="tg-4fps">Auteur</th>
							   <th class="tg-4fps">Lien</th>
							   <th class="tg-4fps">Tags</th>
							   <th class="tg-4fps">Importance</th>
							   <th class="tg-4fps">Datasets</th>
							   <th class="tg-4fps">Code</th>
							 </tr>
						   </thead>
						   <tbody>
							 <tr>
							   <td class="tg-tf2e"><span style="font-weight:500">A Riemannian Network for SPD Matrix Learning</span></td>
							   <td class="tg-tf2e"><span style="font-weight:500">Zhiwu Huang, Luc Van Gool</span></td>
							   <td class="tg-tf2e">https://arxiv.org/abs/1608.04233</td>
							   <td class="tg-tf2e">SPDnet<br>Neural Network<br>Riemannian Geometry</td>
							   <td class="tg-tf2e">↑</td>
							   <td class="tg-tf2e">Emotion recognition<br>Action recognition<br>Face verification</td>
							   <td class="tg-tf2e">https://github.com/zhiwu-huang/SPDNet</td>
							 </tr>
							 <tr>
							   <td class="tg-0lax"><span style="font-weight:500">Riemannian batch normalization for SPD neural networks</span></td>
							   <td class="tg-0lax"><span style="font-weight:500">Daniel A. Brooks et al</span></td>
							   <td class="tg-0lax">https://arxiv.org/pdf/1909.02414.pdf</td>
							   <td class="tg-0lax">SPDnet<br>Riemannian Geometry<br>Batch normalisation</td>
							   <td class="tg-0lax">↑</td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							 </tr>
							 <tr>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">Is Second-order Information Helpful for Large-scale Visual Recognition?</span></td>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">Peihua Li1, Jiangtao Xie1, Qilong Wang1, Wangmeng Zuo2 1Dalian University of Technology, 2Harbin Institute of Technology</span></td>
							   <td class="tg-tf2e">https://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Is_Second-Order_Information_ICCV_2017_paper.pdf</td>
							   <td class="tg-tf2e">Second order network<br>Robust estimation<br>Neural network</td>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">↑</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							 </tr>
							 <tr>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">Neural Architecture Search of SPD Manifold Networks</span></td>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">Rhea Sanjay Sukthanker1, Zhiwu Huang1, Suryansh Kumar1, Erik Goron Endsjo1, Yan Wu1, Luc Van Gool1,2</span></td>
							   <td class="tg-0lax">https://arxiv.org/pdf/2010.14535.pdf</td>
							   <td class="tg-0lax">SPDnet<br>Batch normalisation<br>Architecture discovery</td>
							   <td class="tg-0lax">-</td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							 </tr>
							 <tr>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">Matrix backpropagation for deep net- works with structured layer</span></td>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">Catalin Ionescu et al</span></td>
							   <td class="tg-tf2e">https://openaccess.thecvf.com/content_iccv_2015/papers/Ionescu_Matrix_Backpropagation_for_ICCV_2015_paper.pdf</td>
							   <td class="tg-tf2e">Bilinear Neural network<br>Optimisation<br>Matrix backpropagation</td>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">↑</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							 </tr>
							 <tr>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">ReDro: Efficiently Learning Large-sized SPD Visual Representation</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">↓</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							 </tr>
							 <tr>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">A prototype-based SPD matrix network for domain adaptation EEG emotion recognition</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">↑</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							 </tr>
							 <tr>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">Nonlinear Ranking Loss on Riemannian Potato Embedding</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">↓</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							 </tr>
							 <tr>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">ManifoldNorm: Extending normalizations on Riemannian Manifolds</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e">-</td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							 </tr>
							 <tr>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">SymNet: A Simple Symmetric Positive Definite Manifold Deep Learning Method for Image Set Classification</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax">-</td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							 </tr>
							 <tr>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">Bilinear CNN models for fine-grained visual recognition</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">↑</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							 </tr>
							 <tr>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">DeepKSPD: Learning Kernel-matrix-based SPD Representation for Fine-grained Image Recognition</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">↓</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							 </tr>
							 <tr>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">GeomNet</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"><span style="font-weight:normal;font-style:normal;text-decoration:none">↑</span></td>
							   <td class="tg-tf2e"></td>
							   <td class="tg-tf2e"></td>
							 </tr>
							 <tr>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">Geoopt</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"><span style="font-weight:normal;font-style:normal;text-decoration:none">↓</span></td>
							   <td class="tg-0lax"></td>
							   <td class="tg-0lax"></td>
							 </tr>
						   </tbody>
						   </table>
						</div>
					</section>


				</section>
				<!-- End of second horizontal slide-->

				<section>

					<section>
						<h1>A founding paper (probably) : <a href="https://arxiv.org/abs/1608.04233">SPDnet</a></h1>
						<p>Zhiwu Huang, Luc Van Gool</br>
							Computer Vision Lab, ETH Zurich, Switzerland</br>
							{zhiwu.huang, vangool}@vision.ee.ethz.ch</p>
					</section>

					<section>
						<h2>Architecture</h2>
						<figure>
							<img src="./content/SPDnet.png">
						</figure>
						<b>3 types of new layers :</b>
						<blockquote>
							<ul style="font-size: smaller;">
								<li><b>BiMap</b> : $W$ is the equivalent of weights. Supposed to be orthogonal.</br>
								&rarr; Riemannian backpropagation on Stiefel </li>
								<li><b>ReEig</b> : $\epsilon$ is chosen beforehand and allows regularisation</li>
								<li><b>LogEig</b> : Just a logaritmic mapping to have Euclidean data before classifier</li>
							</ul>
						</blockquote>
					</section>

					<section>
						<h2>AFEW results (haha)</h2>
						<div class="row">
							<div class="col-xs-6">
								<figure>
									<img style="height: 300px;" src="./content/AFEW SPDnet.png">
								</figure>
							</div>
							<div class="col-xs-5">
								<p style="font-size: smaller;">
									The results for the AFEW, HDM05 and PaSC datasets. PaSC1/PaSC2 are the control/handheld testings.
								</p>
							</div>
						</div>

					</section>

					<section>
						<h2>AFEW reservations</h2>
						<blockquote>
							<ul>
								<li>Choice of $\epsilon=\{10^{-4}, 5\times 10^{-5}, 0\}$ : the network probably becomes shallow</li>
								<li>No bias term in the "activation" function</li>
								<li>Pedestrian detection : poor results but may due to tuning</li>
							</ul>
						</blockquote>
						<h2>Remarks</h2>
						<blockquote>
							<ul>
								<li>Matlab/python code repoduce results</li>
								<li>Re-used in many further works (especially EEG kinda)</li>
							</ul>
						</blockquote>
					</section>

				</section>
				<section>
					<section>
						<h1>The (not so much) pre-founding paper : <a href="http://vis-www.cs.umass.edu/bcnn/">BCNN</a> </h1>
						<p>
							Tsung-Yu Lin</br>
							Aruni RoyChowdhury</br>
							Subhransu Maji</br>
						</p>
					</section>

					<section>
						<h2>Riemannian geometry ? We don't do that here</h2>
						<figure>
							<img src="./content/BCNN.png">
						</figure>
						<ul>
							<li>Use bilinear pooling (kinda related to second order) but for a completely different approach.</li>
							<li>$2017< 2018 $ SPDnet so I put that here...</li>
						</ul>
						
					</section>				
				</section>

				<section>
					<section>
						<h1>A new of hope : <a href="https://proceedings.neurips.cc/paper/2019/file/6e69ebbfad976d4637bb4b39de261bf7-Paper.pdf">NEURIPS with minimal increment on SPDnet</a></h1>
						<p>Daniel Brooks, Olivier Schwander, Frédéric Barbaresco, Jean-Yves Schneider, Matthieu Cord</br>
							Thales Land and Air Systems and Sorbonne University </p>
					</section>

					<section>
						<h2>The secret art of batch normalisation in neural networks</h2>
						<figure>
							<img width="70%"" src="./content/batchnorm.png">
							<figcaption style="font-size: small;">Credit : <a href="https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338">Johann Huber</a></figcaption>
						</figure>
						<b>Wiki :</b>
						<blockquote>
							Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling.
						</blockquote>

					</section>

					<section>
						<h2>In (math) layman terms :</h2>
						<p style="font-size: smaller;">
							Let us use $B$ to denote a mini-batch of size $m$ of the entire training set. The empirical mean and variance of $B$ could thus be denoted as
							$$
							\mu_{B}=\frac{1}{m} \sum_{i=1}^{m} x_{i} \text {, and } \sigma_{B}^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2}.
							$$

							For a layer of the network with d-dimensional input, $x=\left(x^{(1)}, \ldots, x^{(d)}\right)$, each dimension of its input is then normalized (i.e. re-centered and re-scaled) separately :
							$$
							\hat{x}_{i}^{(k)}=\frac{x_{i}^{(k)}-\mu_{B}^{(k)}}{\sqrt{\sigma_{B}^{(k)^{2}}+\epsilon}},
							$$
							where $k\in\{1,\dots,d\}$, $i\in\{1,\dots,m\}$, $\mu_{B}^{(k)}$ and $\sigma_{B}^{(k)^{2}}$ are the per-dimension mean and variance, respectively.
						</p>
					</section>

					<section>
						<h2>Have you heard of parallel transport ? (ou transport peremptoire)</h2>
						Transport vectors in tangent plane of $P_1$ to the tangent space of $P_2$ while staying parallel to itself in the tangent space along the path between $P_1$ and $P_2$.</br> Expression for the transport (and actually for the PSD matrix which is surprising) :  
						$$
							\forall S \in \mathcal{T}_{P_{1}}, \Gamma_{P_{1} \rightarrow P_{2}}(S)=\left(P_{2} P_{1}^{-1}\right)^{\frac{1}{2}} S\left(P_{2} P_{1}^{-1}\right)^{\frac{1}{2}} \in \mathcal{T}_{P_{2}}	
						$$
					</section>

					<section>
						<h2>The two commendements of the Yavé (Batch) god</h2>
						<b>Batch centering and biaising</b>
						<blockquote>
							<ul>
								<li>Centering from $\mathcal{G}$ (barycenter of PSD data) : $\forall i \leq N, \bar{P}_{i}=\Gamma_{\mathfrak{G} \rightarrow I_{d}}\left(P_{i}\right)=\mathfrak{G}^{-\frac{1}{2}} P_{i} \mathfrak{G}^{-\frac{1}{2}}$</li>
								<li>Biaising towards parameter $G$ : $\forall i \leq N, \tilde{P}_{i}=\Gamma_{I_{d} \rightarrow G}\left(\bar{P}_{i}\right)=G^{\frac{1}{2}} \bar{P}_{i} G^{\frac{1}{2}}$</li>
							</ul>
						</blockquote>

					</section>

					<section>
						<h2>You shall (not) center, reduce</h2>
						<div class="row">
							<div class="col-xs-6">
								<figure>
									<img height="300px "src="./content/batchnorm algo.png">
									<figcaption style="font-size: smaller;">Very much new algorithm. So much wow !</figcaption>
								</figure>
							</div>
							<div class="col-xs-6">
								The bias parameter matrix $G$ of the RBN is by construction constrained to the SPD manifold.</br>
								&rarr; Riemannian backpropagation</br>(so much original. so much impressed)
						
							</div>
						</div>
					</section>

					<section>
						<h2>AFEW results ANEW (haha..)</h2>
						<figure>
							<img src="./content/AFEW RBN.png">
						</figure>
						
					</section>
				</section>

				<section>
					<section>
						<h1>Robust and too cool for Riemann :  <a href="https://arxiv.org/abs/1703.08050">Second-order neural network</a></h1>
						<p>Peihua Li, Jiangtao Xie, Qilong Wang, Wangmeng Zuo</br>
						   Dalian University of Technology and Harbin Institute of Technology</p>
					</section>

					<section>
						<h2>Streets ahead : Estimating the covariance inside the network</h2>
						<figure>
							<img height="200px" src="./content/MPN-COV.png">
						</figure>
						But what is <b>FC</b> you ask ?
						<blockquote style="font-size: smaller;">
							Fully Connected layers in a neural networks are those layers where all the inputs from one layer are connected to every activation unit of the next layer. In most popular machine learning models, the last few layers are full connected layers which compiles the data extracted by previous layers to form the final output.
						</blockquote>
					</section>

					<section>
						<h2>Covariance : Sample covariance matrix with an attitude</h2>
						Let $\mathbf{X}\in\mathbb{R}^{d\times N}$ a matrix whos columns consist of $N$ features of dimension $d$. Sample covariance matrix $\mathbf{P}$:
						$$
						\mathbf{X} \mapsto \mathbf{P}, \quad \mathbf{P}=\mathbf{X} \overline{\mathbf{I}} \mathbf{X}^{T},
						$$
						where $\overline{\mathbf{I}}=\frac{1}{N}\left(\mathbf{I}-\frac{1}{N} \mathbf{1} \mathbf{1}^{T}\right)$
					</section>

					<section>
						<h2>You said power ? </h2>
						<ul>
							<li>EVD : 
							<blockquote>
								$
								\mathbf{P} \mapsto(\mathbf{U}, \mathbf{\Lambda}), \quad \mathbf{P}=\mathbf{U} \boldsymbol{\Lambda} \mathbf{U}^{T}
								$
							</blockquote></li>
							<li>The power stones (eigenvalues)
								<blockquote>
									$$
(\mathbf{U}, \boldsymbol{\Lambda}) \mapsto \mathbf{Q}, \quad \mathbf{Q} \triangleq \mathbf{P}^{\alpha}=\mathbf{U F}(\boldsymbol{\Lambda}) \mathbf{U}^{T},
$$
$\alpha$ positive scalar and $\mathbf{F}$ defined by $
f\left(\lambda_{i}\right)=\left\{\begin{array}{cc}
\lambda_i ^\alpha & \text{for MPN}\\
\lambda_{i}^{\alpha} / \lambda_{1}^{\alpha} & \text { for MPN+M- } \ell_{2} \\
\lambda_{i}^{\alpha} /\left(\sum_{k} \lambda_{k}^{2 \alpha}\right)^{\frac{1}{2}} & \text { for MPN+M-Fro }
\end{array}\right.
$ 
								</blockquote>
							</li>
						</ul>
					</section>

					<section>
						<h2>Vectorisation is inevitable...</h2>

						<blockquote style="font-size: smaller;">
							Our MPN- COV pooling replaces the common first-order, max/average pooling after the last conv. layer, producing a global, d(d + 1)/2−dimensional image representation by concate- nation of the upper triangular part of one covariance ma- trix. In state-of-the-art ConvNets, the feature dimension d of the last conv. layer gets much larger. For such archi- tectures, we add a 1 × 1 conv. layer of 256 channels after the last conv. layer, so that the dimension of features in- putted to the MPN-COV layer is fixed to 256 (see Sec. 5.3 and Sec. 5.4). As such, we alleviate the problem of small sample of large-dimensional features while decreasing the computational cost of the MPN-COV layer.
						</blockquote>
					</section>

				</section>
	
				<section>
					<section>
						<h1><a href="http://saimunur.github.io/redro_eccv2020.html">ReDro</a> : the prodigal son</h1>
						<p>Saimunur Rahman et al</p>
					</section>

					<section>
						<h2>I am lazy</h2>
						Dropout with SPD Riemannian stuff (relationship dropout). C'est intéressant mais un incrément possible et pas une fin en soi.
					</section>
				</section>

				<section>
					<section>
						<h1><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Nguyen_GeomNet_A_Neural_Network_Based_on_Riemannian_Geometries_of_SPD_ICCV_2021_paper.pdf">Geomnet</a> : so bad yet so good</h1>
						Xuan Son Nguyen</br>
						ETIS UMR 8051, CY Cergy Paris Université, ENSEA, CNRS, F-95000, Cergy, France
					</section>

					<section>
						<h2>Thanks Paul..</h2>

								$$
								\mathbf{P}^{c}=\frac{1}{L-1} \sum_{i=1}^{L} \mathcal{T}_{\mathbf{P}^{m}, \tilde{\mathbf{P}}}\left(\overrightarrow{\mathbf{P}^{m} \mathbf{P}_{i}}\right) \otimes \mathcal{T}_{\mathbf{P}^{m}, \tilde{\mathbf{P}}}\left((\overrightarrow{\mathbf{P}^{m} \mathbf{P}_{i}}\right)
								$$

						<b>Geomnet :</b>
						<ul>
							<li>Focuses on 3D skeletons</li>
							<li>Considers list of SPD matrices. Computes mean and cov, Pennec style</li>

						</ul>
					</section>

					<section>
						<h2>The title of this slide has been lost</h2>

								$$
\left(\mathbf{P}^{m}, \mathbf{P}^{c}\right) \mapsto\left[\begin{array}{cc}
\mathbf{L} & \mathbf{0}_{n^{\prime} \times k^{\prime}} \\
\varphi\left(\mathbf{P}^{m}\right) & \mathbf{I}_{k^{\prime}}
\end{array}\right]
$$

						<b>Geomnet :</b>
						<ul>
							<li>Bonus point: computes the cov around a particular (learned) point via parallel transport (Not sure why it is here, but seems to be crucial)</br>
							&rarr; geomnet_1 et geomnet_3</li>
							<li>Considers the pair of SPDs as a Lie group. Embeds it in a triangular (Cholesky) matrix $\mathbf{B}$.</br>
							&rarr; geomnet_2 : <ul>
								<li>A layer computes BW, with W a Cholesky matrix</li>
								<li>Updates W using Riemannian geom</li>
							</ul></li>

						</ul>
					</section>

					<section>
						<h2>Conclusion</h2>
						<p>Very interesting, but not a threat. Also $\mathbf{B}$ is huge.</p>
						<p>Also some results :</p>
						<figure>
							<img src="content/geomnet_3.PNG">
						</figure>


					</section>
				</section>

<!-- --------------------------------------------------------------------------------------------- -->
<!-- -------------------------- IT ENDS HERE -------------------------- -->
<!-- --------------------------------------------------------------------------------------------- -->
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/menu/menu.js"></script>
		<script src="plugin/chalkboard/plugin.js"></script>
		<script src="plugin/verticator/verticator.js"></script>
		<script src="plugin/presentable/presentable.min.js"></script>
		
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				backgroundTransition: 'slide',
				hash: true,
				touch: true,
				controls: true,
				transition: 'slide',
				dependencies: [
					{ src: 'plugin/reveald3js/reveald3.js' },
					// { src: 'plugin/title-footer/title-footer.js', async: true, callback: function() { title_footer.initialize(); } },
					// { src: 'plugin/typed/typed.js', async: false , callback: function() { callTypedJs(); }}
				],
				math: {
					mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
					config: 'TeX-AMS_HTML-full',
					},
				menu: {
					numbers: true,
					width: 'wide',
					themes: false,
					transitions: false,
					openSlideNumber: true,
				},
				chalkboard: {
					toggleChalkboardButton: { left: "80px" },
					toggleNotesButton: { left: "130px" },
					colorButtons: true,
					boardHandle: true,
					eraser: { src: path + 'img/sponge.png', radius: 40},
					storage: './notes'
				},
				verticator: {
					darktheme: false,
					skipuncounted: false,
					clickable: true
				},
				slideNumber: "c/t",
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMath, RevealMarkdown, RevealHighlight, RevealNotes, RevealMenu, RevealChalkboard, Verticator ]
			});

			presentable.toc({
				framework: "revealjs",
				hideNoTitle: true,
				titles: "h1,.presentable-title"
			});
		</script>
	</body>
</html>
